## Batch Normalization
- Batch normalization is a technique for improving the speed, performance, and stability of artificial neural networks. 
- This slows down the training by requiring lower learning rates
- It helps to reduce the number of epochs required for training

## Data Augmentation
- Data Augmentation helps the neural networks to reduce it from learning irrelevant patterns
- Creates variation in images and improves the training model
- Creates a subset of our training dataset

## Drop Out
- The term “dropout” refers to ignoring units in a neural network during the training phase, which is chosen at random.
- Dropout refers to Dropout is used to prevent over-fitting of training data
- Dropout is an approach to regularization in neural networks which helps to reduce interdependent learning

## Optimizers Used
**Adam**
- Optimization algorithm
- Improved version or update of SGD optimizer for less epoch values
- Computes individual learning rates from different parameters

**Stochastic Gradient Descent (GSD)**
- Optimization algorithm also known as an incremental gradient
- As most SGD techniques run in iterative fashion
- Maintains single learning through different parameters
