Batch Normalization:
•	Batch normalization is a technique for improving the speed, performance, and stability of artificial neural networks. 
•	This slows down the training by requiring lower learning rates
•	It helps to reduce the number of epochs required for training

Data Augmentation
•	Data Augmentation helps the neural networks to reduce it from learning irrelevant patterns
•	Creates variation in images and improves the training model
•	Creates a subset of our training dataset

Drop Out
•	The term “dropout” refers to ignoring units in a neural network during the training phase, which is chosen at random.
•	Dropout refers to Dropout is used to prevent over-fitting of training data
•	Dropout is an approach to regularization in neural networks which helps reducing interdependent learning

Adam
•	Optimization algorithm
•	Improved version or update of SGD optimizer for less epoch values
•	Computes individual learning rates from different parameters

SGD
•	Optimization algorithm also known as an incremental gradient
•	As most SGD techniques run in iterative fashion
•	Maintains single learning through different parameters
